import json
import random
import os
import shutil
from collections import defaultdict

# ======================== é›†ä¸­é…ç½®å‚æ•°ï¼ˆå¯ç›´æ¥ä¿®æ”¹ï¼‰========================
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# åŸæ•°æ®è·¯å¾„
FULL_ANNOTATION_PATH = os.path.join(PROJECT_ROOT, "data", "thumos", "annotations", "thumos14.json")
FULL_FEATURE_DIR = os.path.join(PROJECT_ROOT, "data", "thumos", "i3d_features")
# å°æ•°æ®é›†è¾“å‡ºè·¯å¾„
SMALL_DATA_ROOT = "data/thumos_small"
SMALL_TRAIN_ANNOT = f"{SMALL_DATA_ROOT}/annotations/thumos14_train_small.json"
SMALL_TEST_ANNOT = f"{SMALL_DATA_ROOT}/annotations/thumos14_test_small.json"
SMALL_FEATURE_DIR = f"{SMALL_DATA_ROOT}/i3d_features"

# æŠ½æ ·é…ç½®ï¼ˆäºŒé€‰ä¸€ï¼Œä¿®æ”¹åæ³¨é‡Šæ‰å¦ä¸€ä¸ªï¼‰
SAMPLING_MODE = "COUNT"  # æŠ½æ ·æ¨¡å¼ï¼š"COUNT"ï¼ˆæŒ‰æ•°é‡ï¼‰æˆ– "RATIO"ï¼ˆæŒ‰æ¯”ä¾‹ï¼‰
# æ¨¡å¼1ï¼šæŒ‰æ•°é‡æŠ½æ ·ï¼ˆæ¯ä¸ªç±»åˆ«æŠ½æŒ‡å®šä¸ªæ•°ï¼‰
TRAIN_NUM_PER_CLASS = 3    # æ¯ä¸ªç±»åˆ«è®­ç»ƒæ ·æœ¬æ•°
TEST_NUM_PER_CLASS = 2     # æ¯ä¸ªç±»åˆ«æµ‹è¯•æ ·æœ¬æ•°
# æ¨¡å¼2ï¼šæŒ‰æ¯”ä¾‹æŠ½æ ·ï¼ˆæ¯ä¸ªç±»åˆ«æŠ½æŒ‡å®šæ¯”ä¾‹ï¼‰
TRAIN_RATIO_PER_CLASS = 0.2  # æ¯ä¸ªç±»åˆ«è®­ç»ƒæ ·æœ¬æŠ½æ ·æ¯”ä¾‹ï¼ˆå¦‚ 0.2=20%ï¼‰
TEST_RATIO_PER_CLASS = 0.1   # æ¯ä¸ªç±»åˆ«æµ‹è¯•æ ·æœ¬æŠ½æ ·æ¯”ä¾‹ï¼ˆå¦‚ 0.1=10%ï¼‰

# å…œåº•é™åˆ¶ï¼ˆé¿å…æ ·æœ¬è¿‡å¤šï¼‰
TRAIN_MAX_TOTAL = 100  # è®­ç»ƒé›†æœ€å¤§æ€»æ ·æœ¬æ•°
TEST_MAX_TOTAL = 50    # æµ‹è¯•é›†æœ€å¤§æ€»æ ·æœ¬æ•°
RANDOM_SEED = 42       # å›ºå®šç§å­ï¼ˆä¿è¯æ¯æ¬¡æŠ½æ ·ç»“æœä¸€è‡´ï¼‰
# ==========================================================================


def check_input_paths():
    """æ£€æŸ¥åŸæ•°æ®è·¯å¾„æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(FULL_ANNOTATION_PATH):
        raise FileNotFoundError(f"åŸæ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨ï¼š{FULL_ANNOTATION_PATH}")
    if not os.path.exists(FULL_FEATURE_DIR):
        raise FileNotFoundError(f"åŸç‰¹å¾ç›®å½•ä¸å­˜åœ¨ï¼š{FULL_FEATURE_DIR}")
    print("âœ… åŸæ•°æ®è·¯å¾„æ£€æŸ¥é€šè¿‡")


def parse_full_annotation():
    """è§£ææ ‡æ³¨ï¼Œè¿”å›ï¼š
    - è®­ç»ƒæ± ï¼ˆåŸvalidationï¼‰ã€æµ‹è¯•æ± ï¼ˆåŸtestï¼‰çš„è§†é¢‘ä¿¡æ¯
    - æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬åˆ†å¸ƒï¼ˆè®­ç»ƒæ± +æµ‹è¯•æ± ï¼‰
    - ç±»åˆ«æ˜ å°„
    """
    with open(FULL_ANNOTATION_PATH, "r", encoding="utf-8") as f:
        full_data = json.load(f)

    # å…¼å®¹æ ‡æ³¨æ ¼å¼ï¼ˆdatabase dict æˆ– ç›´æ¥åˆ—è¡¨ï¼‰
    if "database" in full_data and isinstance(full_data["database"], dict):
        database = full_data["database"]
    elif isinstance(full_data, list):
        database = {item.get("video_name", str(i)): item for i, item in enumerate(full_data)}
    else:
        raise ValueError("æ ‡æ³¨æ ¼å¼é”™è¯¯ï¼éœ€å« 'database' å­—æ®µæˆ–ç›´æ¥ä¸ºæ ·æœ¬åˆ—è¡¨")

    # åˆå§‹åŒ–å˜é‡
    train_pool = {}  # è®­ç»ƒæ± ï¼šåŸvalidationè§†é¢‘ {vid: info}
    test_pool = {}   # æµ‹è¯•æ± ï¼šåŸtestè§†é¢‘ {vid: info}
    class_map = {}   # {label_id: label_name}
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬åˆ†å¸ƒï¼ˆè®­ç»ƒæ± +æµ‹è¯•æ± ï¼‰
    class_distribution = {
        "train_pool": defaultdict(list),  # {label_id: [vid1, vid2...]}
        "test_pool": defaultdict(list)    # {label_id: [vid1, vid2...]}
    }

    for vid, info in database.items():
        # æå–subsetï¼ˆå…¼å®¹å¤§å°å†™å’Œå­—æ®µåï¼‰
        subset = info.get("subset", info.get("split", "")).strip().lower()
        annotations = info.get("annotations", [])
        if not annotations:
            continue  # è·³è¿‡æ— æ ‡æ³¨è§†é¢‘

        # æ„å»ºç±»åˆ«æ˜ å°„å’Œåˆ†å¸ƒç»Ÿè®¡
        for ann in annotations:
            label_id = ann.get("label_id")
            label_name = ann.get("label", ann.get("category", f"class_{label_id}"))
            if label_id not in class_map:
                class_map[label_id] = label_name
            # æŒ‰subsetæ·»åŠ åˆ°å¯¹åº”æ± çš„ç±»åˆ«åˆ†å¸ƒä¸­
            if subset == "validation":
                if vid not in class_distribution["train_pool"][label_id]:
                    class_distribution["train_pool"][label_id].append(vid)
            elif subset == "test":
                if vid not in class_distribution["test_pool"][label_id]:
                    class_distribution["test_pool"][label_id].append(vid)

        # æŒ‰subsetæ·»åŠ åˆ°è®­ç»ƒæ± /æµ‹è¯•æ± 
        if subset == "validation":
            train_pool[vid] = info
        elif subset == "test":
            test_pool[vid] = info

    # æ‰“å°åŸæ•°æ®é›†ç±»åˆ«åˆ†å¸ƒï¼ˆæ ¸å¿ƒéœ€æ±‚ï¼‰
    print("\n" + "="*80)
    print("ğŸ“Š åŸæ•°æ®é›†æ¯ä¸ªç±»åˆ«æ ·æœ¬æ•°ç»Ÿè®¡ï¼ˆè®­ç»ƒæ± =åŸvalidationï¼Œæµ‹è¯•æ± =åŸtestï¼‰")
    print("="*80)
    print(f"{'ç±»åˆ«ID':<10} {'ç±»åˆ«åç§°':<20} {'è®­ç»ƒæ± æ ·æœ¬æ•°':<15} {'æµ‹è¯•æ± æ ·æœ¬æ•°':<15}")
    print("-"*80)
    for label_id in sorted(class_map.keys()):
        label_name = class_map[label_id]
        train_cnt = len(class_distribution["train_pool"].get(label_id, []))
        test_cnt = len(class_distribution["test_pool"].get(label_id, []))
        print(f"{label_id:<10} {label_name:<20} {train_cnt:<15} {test_cnt:<15}")
    print("-"*80)
    print(f"{'æ€»è®¡':<10} {'-':<20} {len(train_pool):<15} {len(test_pool):<15}")
    print("="*80 + "\n")

    return train_pool, test_pool, class_map, class_distribution


def sample_small_dataset(class_distribution, pool_type):
    """æ ¹æ®é…ç½®æŠ½æ ·å°æ•°æ®é›†
    Args:
        class_distribution: ç±»åˆ«åˆ†å¸ƒ dict
        pool_type: æ± ç±»å‹ "train_pool" æˆ– "test_pool"
    Returns:
        small_vids: æŠ½æ ·åçš„è§†é¢‘IDåˆ—è¡¨
        sample_log: æŠ½æ ·æ—¥å¿—ï¼ˆæ¯ä¸ªç±»åˆ«æŠ½äº†å¤šå°‘ï¼‰
    """
    small_vids = []
    sample_log = defaultdict(dict)  # {label_id: {"total": åŸæ•°é‡, "sampled": æŠ½æ ·æ•°é‡}}

    for label_id, vids in class_distribution[pool_type].items():
        total = len(vids)
        if total == 0:
            sample_log[label_id] = {"total": 0, "sampled": 0}
            continue

        # æŒ‰æ¨¡å¼æŠ½æ ·
        if SAMPLING_MODE == "COUNT":
            # æŒ‰æ•°é‡æŠ½æ ·ï¼ˆä¸è¶…è¿‡è¯¥ç±»åˆ«æ€»æ•°é‡ï¼‰
            if pool_type == "train_pool":
                sample_num = min(TRAIN_NUM_PER_CLASS, total)
            else:
                sample_num = min(TEST_NUM_PER_CLASS, total)
        else:  # SAMPLING_MODE == "RATIO"
            # æŒ‰æ¯”ä¾‹æŠ½æ ·ï¼ˆå››èˆäº”å…¥ï¼Œæœ€å°‘æŠ½1ä¸ªï¼‰
            if pool_type == "train_pool":
                sample_num = max(1, int(total * TRAIN_RATIO_PER_CLASS))
            else:
                sample_num = max(1, int(total * TEST_RATIO_PER_CLASS))

        # æŠ½æ ·ï¼ˆå›ºå®šç§å­ä¿è¯å¯å¤ç°ï¼‰
        sampled_vids = random.sample(vids, sample_num)
        small_vids.extend(sampled_vids)
        sample_log[label_id] = {"total": total, "sampled": sample_num}

    # å»é‡ + å…œåº•é™åˆ¶ï¼ˆä¸è¶…è¿‡æœ€å¤§æ€»æ ·æœ¬æ•°ï¼‰
    small_vids = list(set(small_vids))
    if pool_type == "train_pool" and len(small_vids) > TRAIN_MAX_TOTAL:
        small_vids = random.sample(small_vids, TRAIN_MAX_TOTAL)
        print(f"âš ï¸  è®­ç»ƒé›†æŠ½æ ·æ•°è¶…è¿‡ä¸Šé™ {TRAIN_MAX_TOTAL}ï¼Œéšæœºæˆªå–åˆ° {TRAIN_MAX_TOTAL} ä¸ª")
    elif pool_type == "test_pool" and len(small_vids) > TEST_MAX_TOTAL:
        small_vids = random.sample(small_vids, TEST_MAX_TOTAL)
        print(f"âš ï¸  æµ‹è¯•é›†æŠ½æ ·æ•°è¶…è¿‡ä¸Šé™ {TEST_MAX_TOTAL}ï¼Œéšæœºæˆªå–åˆ° {TEST_MAX_TOTAL} ä¸ª")

    return small_vids, sample_log


def create_small_annotations():
    """ç”Ÿæˆå°æ•°æ®é›†æ ‡æ³¨ï¼ˆå«æŠ½æ ·æ—¥å¿—ï¼‰"""
    train_pool, test_pool, class_map, class_dist = parse_full_annotation()

    # 1. æŠ½æ ·å°è®­ç»ƒé›†ï¼ˆä»åŸvalidationè®­ç»ƒæ± ï¼‰
    print("[æ­¥éª¤1/2] æŠ½æ ·å°è®­ç»ƒé›†...")
    small_train_vids, train_sample_log = sample_small_dataset(class_dist, "train_pool")

    # 2. æŠ½æ ·å°æµ‹è¯•é›†ï¼ˆä»åŸtestæµ‹è¯•æ± ï¼‰
    print("[æ­¥éª¤2/2] æŠ½æ ·å°æµ‹è¯•é›†...")
    small_test_vids, test_sample_log = sample_small_dataset(class_dist, "test_pool")

    # æ‰“å°æŠ½æ ·æ—¥å¿—ï¼ˆæ ¸å¿ƒéœ€æ±‚ï¼‰
    print("\n" + "="*80)
    print(f"ğŸ“ å°æ•°æ®é›†æŠ½æ ·æ—¥å¿—ï¼ˆç§å­={RANDOM_SEED}ï¼Œæ¨¡å¼={SAMPLING_MODE}ï¼‰")
    print("="*80)
    print(f"{'ç±»åˆ«ID':<10} {'ç±»åˆ«åç§°':<20} {'è®­ç»ƒé›†ï¼ˆåŸvalï¼‰':<25} {'æµ‹è¯•é›†ï¼ˆåŸtestï¼‰':<25}")
    print("-"*80)
    for label_id in sorted(class_map.keys()):
        label_name = class_map[label_id]
        # è®­ç»ƒé›†æŠ½æ ·ä¿¡æ¯
        train_total = train_sample_log[label_id]["total"]
        train_sampled = train_sample_log[label_id]["sampled"]
        train_info = f"åŸ{train_total} â†’ æŠ½{train_sampled}"
        # æµ‹è¯•é›†æŠ½æ ·ä¿¡æ¯
        test_total = test_sample_log[label_id]["total"]
        test_sampled = test_sample_log[label_id]["sampled"]
        test_info = f"åŸ{test_total} â†’ æŠ½{test_sampled}"
        print(f"{label_id:<10} {label_name:<20} {train_info:<25} {test_info:<25}")
    print("-"*80)
    print(f"{'æ€»è®¡':<10} {'-':<20} åŸ{len(train_pool)} â†’ æŠ½{len(small_train_vids)} "
          f"{'':<5} åŸ{len(test_pool)} â†’ æŠ½{len(small_test_vids)}")
    print("="*80 + "\n")

    # æ„å»ºå¹¶ä¿å­˜æ ‡æ³¨æ–‡ä»¶
    small_train_db = {vid: train_pool[vid] for vid in small_train_vids}
    small_test_db = {vid: test_pool[vid] for vid in small_test_vids}
    # ä¿æŒåŸæ ‡æ³¨æ ¼å¼
    small_train_ann = {"version": "Thumos14-30fps", "database": small_train_db}
    small_test_ann = {"version": "Thumos14-30fps", "database": small_test_db}

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(SMALL_TRAIN_ANNOT), exist_ok=True)
    with open(SMALL_TRAIN_ANNOT, "w", encoding="utf-8") as f:
        json.dump(small_train_ann, f, indent=2)
    with open(SMALL_TEST_ANNOT, "w", encoding="utf-8") as f:
        json.dump(small_test_ann, f, indent=2)

    print(f"âœ… å°æ ‡æ³¨æ–‡ä»¶ä¿å­˜å®Œæˆï¼š")
    print(f"   - å°è®­ç»ƒé›†ï¼š{len(small_train_vids)} ä¸ªæ ·æœ¬ â†’ {SMALL_TRAIN_ANNOT}")
    print(f"   - å°æµ‹è¯•é›†ï¼š{len(small_test_vids)} ä¸ªæ ·æœ¬ â†’ {SMALL_TEST_ANNOT}")

    return small_train_vids, small_test_vids


def copy_small_features(small_train_vids, small_test_vids):
    """å¤åˆ¶æŠ½æ ·åçš„ç‰¹å¾æ–‡ä»¶"""
    all_small_vids = list(set(small_train_vids + small_test_vids))
    os.makedirs(SMALL_FEATURE_DIR, exist_ok=True)
    print(f"\nğŸ“¥ å¼€å§‹å¤åˆ¶ {len(all_small_vids)} ä¸ªç‰¹å¾æ–‡ä»¶...")

    copied_cnt = 0
    missing_cnt = 0
    missing_vids = []
    for vid in all_small_vids:
        # å…¼å®¹æ–‡ä»¶åï¼ˆç›´æ¥vid.npy æˆ– å«vidçš„å‰ç¼€æ–‡ä»¶åï¼‰
        src_file = os.path.join(FULL_FEATURE_DIR, f"{vid}.npy")
        if not os.path.exists(src_file):
            for fname in os.listdir(FULL_FEATURE_DIR):
                if fname.endswith(".npy") and vid in fname:
                    src_file = os.path.join(FULL_FEATURE_DIR, fname)
                    break
        # å¤åˆ¶æ–‡ä»¶
        dst_file = os.path.join(SMALL_FEATURE_DIR, os.path.basename(src_file))
        if os.path.exists(src_file):
            shutil.copy(src_file, dst_file)
            copied_cnt += 1
        else:
            missing_cnt += 1
            missing_vids.append(vid)

    # è¾“å‡ºå¤åˆ¶ç»“æœ
    print(f"âœ… ç‰¹å¾å¤åˆ¶å®Œæˆï¼š")
    print(f"   - æˆåŠŸå¤åˆ¶ï¼š{copied_cnt} ä¸ªæ–‡ä»¶")
    if missing_cnt > 0:
        print(f"   - ç¼ºå¤±æ–‡ä»¶ï¼š{missing_cnt} ä¸ªï¼ˆç¤ºä¾‹ï¼š{', '.join(missing_vids[:5])}...ï¼‰")


def main():
    print("="*60)
    print(f"ğŸ“Œ å°æ•°æ®é›†ç”Ÿæˆå·¥å…·ï¼ˆæ¨¡å¼ï¼š{SAMPLING_MODE}ï¼Œç§å­ï¼š{RANDOM_SEED}ï¼‰")
    print("="*60)

    # å›ºå®šç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰
    random.seed(RANDOM_SEED)

    try:
        # æ­¥éª¤1ï¼šæ£€æŸ¥è·¯å¾„
        check_input_paths()
        # æ­¥éª¤2ï¼šç”Ÿæˆå°æ ‡æ³¨
        small_train_vids, small_test_vids = create_small_annotations()
        # æ­¥éª¤3ï¼šå¤åˆ¶ç‰¹å¾æ–‡ä»¶
        copy_small_features(small_train_vids, small_test_vids)

        # æœ€ç»ˆæç¤º
        print("\n" + "="*60)
        print("ğŸ‰ å°æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼")
        print(f"ğŸ“ å°æ•°æ®é›†ç›®å½•ï¼š{SMALL_DATA_ROOT}")
        print("ğŸ’¡ é…ç½®æ–‡ä»¶ä¿®æ”¹å‚è€ƒï¼š")
        print(f"   json_file: {SMALL_TRAIN_ANNOT}ï¼ˆè®­ç»ƒï¼‰ / {SMALL_TEST_ANNOT}ï¼ˆæµ‹è¯•ï¼‰")
        print(f"   feat_folder: {SMALL_FEATURE_DIR}")
        print("="*60)
    except Exception as e:
        print(f"\nâŒ ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        raise


if __name__ == "__main__":
    main()